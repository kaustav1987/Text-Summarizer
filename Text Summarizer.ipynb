{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "##\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "from pickle import dump,load\n",
    "import contractions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "##\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,LSTM,Dropout,Input,Activation,Add,concatenate, Embedding, RepeatVector\n",
    "from keras.layers.advanced_activations import LeakyReLU,PReLU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaustav\\Anaconda3\\envs\\keras\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId ProfileName  HelpfulnessNumerator  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW  delmartian                     1   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK      dll pa                     0   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time                Summary  \\\n",
       "0                       1      5  1303862400  Good Quality Dog Food   \n",
       "1                       0      1  1346976000      Not as Advertised   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv('Reviews.csv')\n",
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                         0\n",
      "ProductId                  0\n",
      "UserId                     0\n",
      "ProfileName               16\n",
      "HelpfulnessNumerator       0\n",
      "HelpfulnessDenominator     0\n",
      "Score                      0\n",
      "Time                       0\n",
      "Summary                   27\n",
      "Text                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(reviews.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 27 records summary is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
    "       'HelpfulnessDenominator', 'Score', 'Time'], axis =1, inplace = True)\n",
    "\n",
    "reviews.dropna(inplace = True)\n",
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text   ::::: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "Summary::::: Good Quality Dog Food \n",
      "\n",
      "Text   ::::: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "Summary::::: Not as Advertised \n",
      "\n",
      "Text   ::::: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "Summary::::: \"Delight\" says it all \n",
      "\n",
      "Text   ::::: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
      "Summary::::: Cough Medicine \n",
      "\n",
      "Text   ::::: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
      "Summary::::: Great taffy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    print('Text   :::::',reviews['Text'].iloc[i])\n",
    "    print('Summary:::::',reviews['Summary'].iloc[i], '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''\n",
    "    Preprocess the text \n",
    "    '''\n",
    "    ##Will add to preprocess Class Later\n",
    "    text = text.lower()\n",
    "    ## Expand the contractions\n",
    "    ## You can use a dictionary to do the same job. But I am lazy\n",
    "    text = contractions.fix(text) \n",
    "    for word in text:\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags =re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href',' ',text)\n",
    "        text = re.sub(r'&amp;','',text)\n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]',' ',text)\n",
    "        text = re.sub(r'<br />',' ',text)\n",
    "        text = re.sub(r'\\'',' ',text)\n",
    "    \n",
    "    if remove_stopwords == True:\n",
    "        text = text.split()\n",
    "        text = ' '.join([word for word in text if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78981c32871647f48b21008aa38a0112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=568427), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Summaries clean processed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d1fcc848c4403c858ee1cd2d98f95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=568427), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Text clean processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_clean_text(reviews):\n",
    "    '''\n",
    "    Clean the summaries and text\n",
    "    '''\n",
    "    # Clean the summaries\n",
    "    clean_summaries = []\n",
    "    \n",
    "    ##for summary in reviews['Summary']:\n",
    "    for summary in tqdm_notebook(reviews['Summary'], total= len(reviews['Summary'])):\n",
    "        clean_summaries.append(clean_text(summary,remove_stopwords = False ))\n",
    "    \n",
    "    print(' Summaries clean processed')\n",
    "    # Clean the texts\n",
    "    text_reviews = []\n",
    "    ##for single_text in reviews['Text']:\n",
    "    for single_text in tqdm_notebook(reviews['Text'], total= len(reviews['Text'])):\n",
    "        text_reviews.append(clean_text(single_text,remove_stopwords = True))\n",
    "    \n",
    "    print(' Text clean processed')\n",
    "    return text_reviews, clean_summaries\n",
    "\n",
    "text_reviews, clean_summaries = get_clean_text(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored the Cleaned File\n"
     ]
    }
   ],
   "source": [
    "def store_clean_texts(text_reviews,clean_summaries):\n",
    "    '''\n",
    "    Store Clean Texts ans summaries\n",
    "    '''\n",
    "    #clean_df = pd.DataFrame()\n",
    "    #clean_df['Summary'] = clean_summaries\n",
    "    #clean_df['Text'] = text_reviews \n",
    "    #clean_df.to_csv('clean_df.csv', index= False)\n",
    "    ###\n",
    "    stories = list()\n",
    "    for i, text in enumerate(text_reviews):\n",
    "        stories.append({'story': text, 'highlights':clean_summaries[i] })\n",
    "    \n",
    "    # save to file\n",
    "    dump(stories,open('D:\\Project data\\Text Summarizer\\Amazon Food Review/review_dataset.pkl','wb'))\n",
    "    print('Stored the Cleaned File')\n",
    "    \n",
    "\n",
    "store_clean_texts(text_reviews,clean_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from  here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperParameter Set\n"
     ]
    }
   ],
   "source": [
    "def hyperparameters():\n",
    "    batch_size = 64\n",
    "    epochs = 10\n",
    "    latent_dim = 256\n",
    "    num_samples = 10000\n",
    "    VOCAB_SIZE = 50000\n",
    "    SUMMARY_VOCAB_SIZE = 46615\n",
    "    text_pad_length = 117\n",
    "    summary_pad_length = 9\n",
    "    ##\n",
    "    MAX_LEN = 1000\n",
    "    VOCAB_SIZE =15000\n",
    "    EMBEDDING_DIM = 300\n",
    "    HIDDEN_UNITS = 200\n",
    "    #VOCAB_SIZE = VOCAB_SIZE + 1\n",
    "\n",
    "    LEARNING_RATE = 0.002\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 5\n",
    "    \n",
    "    print('HyperParameter Set')\n",
    "    return batch_size,epochs,latent_dim,num_samples, VOCAB_SIZE,text_pad_length,summary_pad_length,SUMMARY_VOCAB_SIZE,\\\n",
    "           LEARNING_RATE,BATCH_SIZE, EPOCHS\n",
    "\n",
    "\n",
    "batch_size,epochs,latent_dim,num_samples, VOCAB_SIZE,text_pad_length,summary_pad_length,SUMMARY_VOCAB_SIZE,\\\n",
    "           LEARNING_RATE,BATCH_SIZE, EPOCHS = hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_texts(path='D:\\Project data\\Text Summarizer\\Amazon Food Review', file_name='review_dataset.pkl'):\n",
    "    stories = load(open(os.path.join(path, file_name), 'rb'))\n",
    "    \n",
    "    print('No of Stories Loaded', len(stories))\n",
    "    return stories\n",
    "\n",
    "#stories = load_clean_texts(path='D:\\Project data\\Text Summarizer\\Amazon Food Review', file_name='review_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Stories Loaded 568427\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c37f3ff06948cdb5887fcbd0bee1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=568427), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_text(path='D:\\Project data\\Text Summarizer\\Amazon Food Review', file_name='review_dataset.pkl'):\n",
    "    '''\n",
    "    Load the Text and the Summaries to continue\n",
    "    '''\n",
    "    ##Load Stories\n",
    "    stories = load_clean_texts(path='D:\\Project data\\Text Summarizer\\Amazon Food Review', file_name='review_dataset.pkl')\n",
    "    ##Stories loaded\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    ##for story in stories:\n",
    "    for story in tqdm_notebook(stories, total =len(stories)):\n",
    "        input_text = story['story']\n",
    "        target_text = story['highlights']\n",
    "        \n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "    ##    \n",
    "    clean_df = pd.DataFrame()\n",
    "    clean_df['Text'] = input_texts \n",
    "    clean_df['Summary'] = target_texts\n",
    "    clean_df['Text_length']  = [len(x.split()) for x in clean_df['Text']]\n",
    "    clean_df['Summary_length']  = [len(x.split()) for x in clean_df['Summary']]\n",
    "    return clean_df\n",
    "    \n",
    "clean_df = get_text(path='D:\\Project data\\Text Summarizer\\Amazon Food Review', file_name='review_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text:\n",
      "count    568427.000000\n",
      "mean         42.196340\n",
      "std          43.582674\n",
      "min           0.000000\n",
      "5%           10.000000\n",
      "25%          17.000000\n",
      "50%          29.000000\n",
      "75%          51.000000\n",
      "95%         117.000000\n",
      "98%         167.000000\n",
      "99%         211.000000\n",
      "max        2114.000000\n",
      "Name: Text_length, dtype: float64\n",
      "\n",
      "\n",
      "Length of Summary:\n",
      "count    568427.000000\n",
      "mean          4.183987\n",
      "std           2.660351\n",
      "min           0.000000\n",
      "5%            1.000000\n",
      "25%           2.000000\n",
      "50%           4.000000\n",
      "75%           5.000000\n",
      "95%           9.000000\n",
      "98%          11.000000\n",
      "99%          13.000000\n",
      "max          48.000000\n",
      "Name: Summary_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Length of Text:')\n",
    "print(clean_df['Text_length'].describe(percentiles = [0.05, .25, .5, .75, .95, .98, .99]))\n",
    "##\n",
    "print('\\n')\n",
    "print('Length of Summary:')\n",
    "print(clean_df['Summary_length'].describe(percentiles = [0.05, .25, .5, .75, .95, .98, .99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I will take the encoder length of 117 ( text length) and decoder length of 9 (Summary Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.drop(['Text_length', 'Summary_length'],axis = 1, inplace =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_summary(clean_df):\n",
    "    input_texts = clean_df['Text']\n",
    "    input_summary = clean_df['Summary']\n",
    "    return input_texts,input_summary\n",
    "##\n",
    "\n",
    "input_texts,input_summary= get_text_and_summary(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tokenizer has some constraints. We may overcome it using Gensim. But Here I am using manual tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed =1987\n",
    "\n",
    "\n",
    "def get_train_valid_set(clean_df):\n",
    "    '''\n",
    "    Split the Data to train and Validation Set\n",
    "    '''\n",
    "    from sklearn.utils import shuffle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    clean_df = shuffle(clean_df)\n",
    "    \n",
    "    ##\n",
    "    input_texts,input_summary= get_text_and_summary(clean_df)\n",
    "    train_input_texts = input_texts[10000:]\n",
    "    valid_input_texts = input_texts[:10000]\n",
    "    train_input_summary = input_texts[10000:]\n",
    "    valid_input_summary = input_texts[:10000]\n",
    "    return train_input_texts,valid_input_texts,train_input_summary,valid_input_summary\n",
    "\n",
    "\n",
    "train_input_texts,valid_input_texts,train_input_summary,valid_input_summary = get_train_valid_set(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568427"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558427"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_and_summary(train_input_texts,valid_input_texts,train_input_summary,valid_input_summary):\n",
    "    text_tokenizer = Tokenizer(num_words=VOCAB_SIZE) ## Set by hyperparameters\n",
    "    text_tokenizer.fit_on_texts(train_input_texts)\n",
    "    ##print(len(text_tokenizer.word_index))\n",
    "    X_train_seq = text_tokenizer.texts_to_sequences(train_input_texts)\n",
    "    X_valid_seq = text_tokenizer.texts_to_sequences(valid_input_texts)\n",
    "    ## I verified only 46615 words are there where word_count is > 3 for my data\n",
    "    summary_tokenizer = Tokenizer(num_words=SUMMARY_VOCAB_SIZE) \n",
    "    summary_tokenizer.fit_on_texts(train_input_summary)\n",
    "    ##print(len(text_tokenizer.word_index))\n",
    "    ##len(tokenizer.word_index)\n",
    "    X_train_summary_seq = summary_tokenizer.texts_to_sequences(train_input_summary)\n",
    "    X_valid_summary_seq = summary_tokenizer.texts_to_sequences(valid_input_summary)\n",
    "    return text_tokenizer,summary_tokenizer,X_train_seq,X_valid_seq,X_train_summary_seq,X_valid_summary_seq\n",
    "\n",
    "\n",
    "text_tokenizer,summary_tokenizer,X_train_seq,X_valid_seq,X_train_summary_seq,X_valid_summary_seq = \\\n",
    "    tokenize_text_and_summary(train_input_texts,valid_input_texts,train_input_summary,valid_input_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "text_word_index_VOCAB_SIZE = {}\n",
    "word_index = text_tokenizer.word_index\n",
    "for word in word_index.keys():\n",
    "    if word_index[word] == 0:\n",
    "        print('Found 0!!')\n",
    "    elif  word_index[word] > VOCAB_SIZE:\n",
    "        continue\n",
    "    else:\n",
    "        text_word_index_VOCAB_SIZE[word] = word_index[word]\n",
    "    \n",
    "print(len(text_word_index_VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46615\n"
     ]
    }
   ],
   "source": [
    "summary_word_index_VOCAB_SIZE = {}\n",
    "word_index = summary_tokenizer.word_index\n",
    "for word in word_index.keys():\n",
    "    if word_index[word] == 0:\n",
    "        print('Found 0!!')\n",
    "    elif  word_index[word] > SUMMARY_VOCAB_SIZE:\n",
    "        continue\n",
    "    else:\n",
    "        summary_word_index_VOCAB_SIZE[word] = word_index[word]\n",
    "    \n",
    "print(len(summary_word_index_VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109992\n"
     ]
    }
   ],
   "source": [
    "summary_word_index = summary_tokenizer.word_index\n",
    "print(len(summary_word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_output(X_train_seq,X_valid_seq,X_train_summary_seq,X_valid_summary_seq):\n",
    "    '''\n",
    "    Pad the Text Sequences and Summary Sequences \n",
    "    '''\n",
    "    X_train_seq_pad = pad_sequences(X_train_seq, maxlen = text_pad_length, padding = 'post', truncating = 'post')\n",
    "    X_valid_seq_pad = pad_sequences(X_valid_seq, maxlen = text_pad_length, padding = 'post', truncating = 'post')\n",
    "    X_train_summary_seq_pad = pad_sequences(X_train_summary_seq, maxlen = summary_pad_length, padding = 'post',\\\n",
    "                                            truncating = 'post')\n",
    "    X_valid_summary_seq_pad = pad_sequences(X_valid_summary_seq, maxlen = summary_pad_length, padding = 'post',\\\n",
    "                                    truncating = 'post')\n",
    "    return X_train_seq_pad,X_valid_seq_pad,X_train_summary_seq_pad,X_valid_summary_seq_pad\n",
    "\n",
    "\n",
    "X_train_seq_pad,X_valid_seq_pad,X_train_summary_seq_pad,X_valid_summary_seq_pad = \\\n",
    "            pad_output(X_train_seq,X_valid_seq,X_train_summary_seq,X_valid_summary_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 9)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq_pad.shape\n",
    "X_valid_seq_pad.shape\n",
    "X_train_summary_seq_pad.shape\n",
    "X_valid_summary_seq_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pretrained glove300D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embedding(path= 'D:\\Project data\\glove_vectors\\glove.6B.300d.txt', encoding = 'utf-8'):\n",
    "    embeddings_index = {}\n",
    "    with open(path, encoding = encoding) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                coef = np.asarray(values[1:], dtype = 'float32')\n",
    "                embeddings_index[word] = coef\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Number of words missing from CN: 11847\n",
      "Percent of words that are missing from vocabulary: 23.69%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "def  missing_word_analysis(text_word_index_VOCAB_SIZE,embeddings_index):\n",
    "    missing_words = 0\n",
    "    threshold = 15\n",
    "    for word, count in text_word_index_VOCAB_SIZE.items():\n",
    "        if count > threshold:\n",
    "            if word not in embeddings_index:\n",
    "                missing_words +=1\n",
    "            \n",
    "    missing_ratio = round(missing_words/len(text_word_index_VOCAB_SIZE),4)*100\n",
    "\n",
    "    print(\"Number of words missing from CN:\", missing_words)\n",
    "    print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))\n",
    "    \n",
    "##\n",
    "embeddings_index = load_pretrained_embedding(path= 'D:\\Project data\\glove_vectors\\glove.6B.300d.txt', \n",
    "                                             encoding = 'utf-8')\n",
    "missing_word_analysis(text_word_index_VOCAB_SIZE,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 10289\n",
      "Percent of words that are missing from vocabulary: 22.07%\n"
     ]
    }
   ],
   "source": [
    "missing_word_analysis(summary_word_index_VOCAB_SIZE,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 417195 word vectors.\n",
      "Number of words missing from CN: 13835\n",
      "Percent of words that are missing from vocabulary: 27.67%\n"
     ]
    }
   ],
   "source": [
    "##Using conceptnet pretrained weights\n",
    "embeddings_index = load_pretrained_embedding(path= 'D:\\Project data\\glove_vectors\\conceptnet_numberbatch_en.txt', \n",
    "                                             encoding = 'utf-8')\n",
    "missing_word_analysis(text_word_index_VOCAB_SIZE,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sorted(tokenizer.word_counts))\n",
    "#print(tokenizer.document_count)\n",
    "#print(tokenizer.word_index)\n",
    "#print(tokenizer.word_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50001, 300)\n",
      "(46616, 300)\n"
     ]
    }
   ],
   "source": [
    "def embedding_matrix_creater(embedding_dimention, word_index,embeddings_index):\n",
    "    '''\n",
    "    Build the embedding matrix using pretrained weights\n",
    "    Use zeros if the word is not there in pretrained vectors\n",
    "    '''\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dimention))\n",
    "    for word, index in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "##\n",
    "embedding_dimention = 300\n",
    "text_embedding_matrix   = embedding_matrix_creater(embedding_dimention, text_word_index_VOCAB_SIZE,embeddings_index)\n",
    "print(text_embedding_matrix.shape)\n",
    "summary_embedding_matrix= embedding_matrix_creater(embedding_dimention, summary_word_index_VOCAB_SIZE,embeddings_index)\n",
    "print(summary_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embedding_layer = Embedding(input_dim = 50001, \n",
    "                                    output_dim = 300,\n",
    "                                    input_length = text_pad_length, ###MAX_LEN,\n",
    "                                    weights = [text_embedding_matrix],\n",
    "                                    trainable = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embedding_layer = Embedding(input_dim = 109993, \n",
    "                                    output_dim = 300,\n",
    "                                    input_length = summary_pad_length, ##MAX_LEN,\n",
    "                                    weights = [summary_embedding_matrix],\n",
    "                                    trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "\n",
    "HIDDEN_UNITS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1. Simple LSTM Encoder-Decoder-seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 117)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 117, 300)     15000300    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 9, 300)       32997900    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 200)          400800      embedding_1[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 200)          400800      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 400)          0           lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 46616)        18693016    concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 67,492,816\n",
      "Trainable params: 19,494,616\n",
      "Non-trainable params: 47,998,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple LSTM Encoder-Decoder-seq2seq\n",
    "\"\"\"\n",
    "# encoder\n",
    "encoder_inputs = Input(shape=(text_pad_length, ), dtype='int32',)\n",
    "encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_UNITS)(encoder_embedding)\n",
    "# decoder\n",
    "decoder_inputs = Input(shape=(summary_pad_length, ))\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(HIDDEN_UNITS)(decoder_embedding)\n",
    "# merge\n",
    "merge_layer = concatenate([encoder_LSTM, decoder_LSTM])\n",
    "decoder_outputs = Dense(units=SUMMARY_VOCAB_SIZE+1, activation=\"softmax\")(merge_layer) # SUM_VOCAB_SIZE, sum_embedding_matrix.shape[1]\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_pad,X_valid_seq_pad,X_train_summary_seq_pad,X_valid_summary_seq_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit([X_train_seq_pad, X_train_summary_seq_pad], \n",
    "                     target_train, \n",
    "                     epochs=EPOCHS, \n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     validation_data=([X_valid_seq_pad, X_valid_summary_seq_pad], target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2. Bidirectional LSTM Encoder-Decoder-seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Bidirectional LSTM: Others Inspired Encoder-Decoder-seq2seq\n",
    "\"\"\"\n",
    "encoder_inputs = Input(shape=(MAX_LEN,))\n",
    "encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True)\n",
    "encoder_LSTM_R = LSTM(HIDDEN_UNITS, return_state=True, go_backwards=True)\n",
    "encoder_outputs_R, state_h_R, state_c_R = encoder_LSTM_R(encoder_embedding)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "\n",
    "final_h = Add()([state_h, state_h_R])\n",
    "final_c = Add()([state_c, state_c_R])\n",
    "encoder_states = [final_h, final_c]\n",
    "\n",
    "\"\"\"\n",
    "decoder\n",
    "\"\"\"\n",
    "decoder_inputs = Input(shape=(MAX_LEN,))\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(HIDDEN_UNITS, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states) \n",
    "decoder_dense = Dense(VOCAB_SIZE, activation='linear')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3. Chatbot Inspired Encoder-Decoder-seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chatbot Inspired Encoder-Decoder-seq2seq\n",
    "\"\"\"\n",
    "encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "\n",
    "decoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True, return_sequences=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "# dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
    "outputs = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = RMSprop(lr=0.01, clipnorm=1.)\n",
    "model.compile(loss='mse', optimizer=rmsprop, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model 2\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate 1: One-Shot Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_txt_length = ...\n",
    "sum_txt_length = ...\n",
    "# encoder input model\n",
    "inputs = Input(shape=(src_txt_length,))\n",
    "encoder1 = Embedding(vocab_size, 128)(inputs)\n",
    "encoder2 = LSTM(128)(encoder1)\n",
    "encoder3 = RepeatVector(sum_txt_length)(encoder2)\n",
    "# decoder output model\n",
    "decoder1 = LSTM(128, return_sequences=True)(encoder3)\n",
    "outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder1)\n",
    "# tie it together\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
